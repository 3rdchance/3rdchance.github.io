1. kaggle have practice of linear regression

develope a comfortable time routine
1. every night 30 minutes about a coding study
- Looking at your data by hand is a good exercise regardless of how you obtained your data. Andrej Karpathy did this on ImageNet and wrote about the experience. 
What I learned from competing against a ConvNet on ImageNet
http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/


* 机器学习中的迭代算法

在机器学习中，**迭代算法**是一类通过逐步优化参数或模型结构来逼近最优解的算法。以下是常见的迭代算法分类及代表性方法，涵盖监督学习、无监督学习和优化领域：

---

### 一、**监督学习中的迭代算法**
#### 1. **梯度下降法（Gradient Descent）**
   - **思想**：沿损失函数的负梯度方向迭代更新参数。
   - **变体**：
     - **批量梯度下降（BGD）**：每次迭代使用全部数据计算梯度。
     - **随机梯度下降（SGD）**：每次随机选择一个样本更新参数（速度快，波动大）。
     - **小批量梯度下降（Mini-batch SGD）**：折中方案（常用 batch_size=32/64）。
   - **应用**：线性回归、神经网络训练。

#### 2. **牛顿法（Newton's Method）**
   - **思想**：利用二阶导数（Hessian 矩阵）加速收敛。
   - **特点**：收敛快但计算成本高，适用于参数较少的问题。
   - **改进**：拟牛顿法（如 **BFGS**、**L-BFGS**），用近似矩阵替代 Hessian。

#### 3. **动量法（Momentum）**
   - **思想**：引入动量项加速收敛并减少震荡。
   - **公式**：
     \[
     v_t = \beta v_{t-1} + (1-\beta) \nabla_\theta J(\theta)
     \]
     \[
     \theta = \theta - \alpha v_t
     \]
   - **应用**：深度学习优化（如 **SGD with Momentum**）。

#### 4. **自适应学习率算法**
   - **Adagrad**：为每个参数分配不同的学习率（适合稀疏数据）。
   - **RMSprop**：改进 Adagrad 的学习率衰减问题。
   - **Adam**：结合动量与自适应学习率（最常用）。
     \[
     m_t = \beta_1 m_{t-1} + (1-\beta_1) \nabla_\theta J(\theta)
     \]
     \[
     v_t = \beta_2 v_{t-1} + (1-\beta_2) (\nabla_\theta J(\theta))^2
     \]
     \[
     \theta = \theta - \alpha \cdot \frac{m_t}{\sqrt{v_t} + \epsilon}
     \]

---

### 二、**无监督学习中的迭代算法**
#### 1. **K-Means 聚类**
   - **步骤**：
     1. 随机初始化 K 个中心点。
     2. 分配每个样本到最近的中心点。
     3. 重新计算中心点（均值）。
     4. 重复 2~3 步直到收敛。
   - **应用**：客户分群、图像压缩。

#### 2. **EM 算法（Expectation-Maximization）**
   - **思想**：通过交替迭代求解隐变量模型的参数。
     - **E 步**：计算隐变量的期望。
     - **M 步**：最大化似然函数更新参数。
   - **应用**：高斯混合模型（GMM）、HMM 训练。

#### 3. **PCA（迭代求解）**
   - **幂迭代法（Power Iteration）**：逐步计算协方差矩阵的主特征向量。
   - **应用**：数据降维、特征提取。

---

### 三、**集成学习中的迭代算法**
#### 1. **Boosting 方法**
   - **AdaBoost**：
     1. 初始样本权重均匀分布。
     2. 迭代训练弱分类器并调整样本权重（增加错误样本权重）。
     3. 组合弱分类器为强分类器。
   - **梯度提升（GBDT、XGBoost、LightGBM）**：
     - 通过梯度下降迭代拟合残差。
     - **XGBoost 改进**：二阶导数优化 + 正则化。

#### 2. **随机森林（Random Forest）**
   - **思想**：通过 Bootstrap 采样迭代生成多棵决策树，投票表决结果。

---

### 四、**深度学习中的特殊迭代方法**
#### 1. **反向传播（Backpropagation）**
   - **步骤**：
     1. 前向传播计算预测值。
     2. 反向传播误差并更新权重（链式法则）。
     3. 迭代至收敛。

#### 2. **迭代式剪枝（Iterative Pruning）**
   - **思想**：逐步移除神经网络中不重要的连接或神经元。
   - **应用**：模型压缩、加速推理。

---

### 五、**优化问题的迭代算法**
#### 1. **坐标下降法（Coordinate Descent）**
   - **思想**：每次迭代仅优化一个参数维度。
   - **应用**：Lasso 回归（线性模型 + L1 正则化）。

#### 2. **近端梯度法（Proximal Gradient）**
   - **思想**：处理不可微正则项（如 L1 正则）的迭代优化。
   - **应用**：稀疏模型训练。

---

### 六、**迭代算法的关键参数**
| 参数                | 作用                              | 典型值                |
|---------------------|----------------------------------|----------------------|
| **学习率（α）**     | 控制步长                          | 0.001~0.1（需调参）  |
| **迭代次数**        | 最大迭代轮数                      | 100~10,000           |
| **容忍度（Tol）**   | 早停阈值（损失变化小于 Tol 停止） | 1e-5~1e-3            |
| **批量大小**        | Mini-batch 的样本数               | 32/64/128            |

---

### 七、**迭代算法的选择建议**
1. **小规模数据**：牛顿法/L-BFGS（收敛快）。
2. **深度学习**：Adam（自适应学习率）或 SGD+Momentum。
3. **稀疏数据**：Adagrad 或 FTRL（在线学习）。
4. **非凸问题**：避免陷入局部最优，可尝试模拟退火（SA）或遗传算法（GA）。

---

### 总结
迭代算法是机器学习的核心，不同方法适用于不同场景：
- **梯度类算法**：主导深度学习优化。
- **EM/Boosting**：解决隐变量或弱模型集成问题。
- **K-Means/PCA**：经典无监督学习工具。  
实际应用中需结合数据规模、模型复杂度和计算资源综合选择。


* Journal - recording
09: 40 linux kernel's interrupts
 1. Interrupt Handling
   a. interrupt vs exception
   b. interupt types
   1.1 I/O Interrupt Handling 
     a. interrupt handler flexibility
     b. action's urgency in interrupt handler
10:11 - 中间分心有些多
     c. 4 basic operations in interrupt handlers
   1.2 interrupt vectors
   1.3 irq data structures
       The irq_desc_t descriptor (faulty hardware, status, how disable)
   1.3 interrupt setup
   1.4 multiple PIC support
   1.5 irq_stats array
   1.6 IRQ distribution in multiprocessor systems
   1.7 Multiple Kernel stack
13:00 - 14:00
   1.8 Saving Register for Interrupt Handlers
   1.9 The do_IRQ() function
   1.10 The __do_IRQ() function
   1.11 Interrupt service routines
   1.11 Dynamic allocation of IRQ lines
2. Interprocessor Interrupt Handling
      
 Questions
   如何做到均衡？ 计算任务，存储任务... kirqd 如何实现？
 How detach process child
    显示ignore SIG_CHILD


* web pages

http://www.unixwiz.net/techtips/win32-callconv-asm.html

https://github.com/FabioBaroni/awesome-exploit-development


https://read.seas.harvard.edu/cs1610/2025/schedule/

https://make.mad-scientist.net/papers/advanced-auto-dependency-generation/


https://zotero-chinese.com/user-guide/sync

https://waynerv.com/posts/how-tty-system-work/s


- random select algorithm
https://juejin.cn/post/6844904096013484040
https://www.keithschwarz.com/darts-dice-coins/

* 2025-0408
1. completion why ?
2. rcu

fill-column

- perfbook
https://www.kernel.org/pub/linux/kernel/people/paulmck/perfbook/perfbook.html

origin  https://github.com/paulmckrcu/perfbook.git (push)



* virtualbox of mac
* slow work



* to paper
1. 3rdchance begin
2. who you are: left life over LJ, LQD and LYH


- memory order
  1.1 perf book
  1.2 gcc manual





C-h t
* 2025-04-14
bias y-intercept

- idol research
